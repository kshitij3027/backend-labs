# Load Generator & Performance Benchmark System

A load generator that stress-tests an asyncio TCP log server with NDJSON protocol and produces JSON benchmark reports.

## Tech Stack

- **Language:** Python 3.12
- **Networking:** asyncio TCP server + client (`asyncio.start_server`, `asyncio.open_connection`)
- **Protocol:** NDJSON over TCP (newline-delimited JSON, one message per line)
- **Security:** TLS support via self-signed certificates (OpenSSL)
- **Compression:** gzip for message payloads
- **Resilience:** Circuit breaker pattern for overload protection
- **Concurrency:** Connection pooling via `asyncio.Queue`
- **Containers:** Docker + Docker Compose

## Architecture

The system consists of three components that work together:

### Async TCP Server (`server/`)

An asyncio-based TCP log ingestion server. Accepts persistent connections, reads NDJSON messages line-by-line, and responds with JSON acknowledgments. Key features:

- **asyncio.start_server** -- single-threaded event loop handling thousands of concurrent connections
- **Optional TLS** -- self-signed certificates generated by `generate_certs.sh`
- **NDJSON message handling** -- each line is a JSON object with `level` and `message` fields
- **gzip decompression** -- transparently decompresses gzip-encoded payloads
- **Circuit breaker** -- trips open after configurable failure thresholds to prevent cascade failures
- **Batched log persistence** -- buffers incoming messages and flushes every N messages or M milliseconds (whichever comes first)
- **Optional UDP transport** -- secondary UDP listener on a separate port

### Load Generator (`generator/`)

An async CLI tool that hammers the server with configurable log traffic. Key features:

- **Async workers** -- fires `TOTAL_LOGS` messages with up to `CONCURRENCY` in-flight via `asyncio.Semaphore`
- **Connection pool** -- `asyncio.Queue`-backed pool that reuses TCP connections across workers, eliminating per-message handshake overhead
- **Duration limit** -- stops after `DURATION_SECS` even if `TOTAL_LOGS` is not reached
- **Real-time progress** -- prints RPS, error count, and error rate every second
- **Latency percentiles** -- computes P50, P95, P99 from per-message round-trip times
- **gzip compression** -- optional payload compression via `--compress`

### Benchmark Tool (`benchmark/`)

A progressive test suite that runs five tiers of increasing load, monitors system resources, and outputs a JSON report. The tiers are:

| Tier | Name | Total Logs | Duration | Concurrency |
|------|------|-----------|----------|-------------|
| 1 | warmup | 100 | 5s | 2 |
| 2 | baseline | 1,000 | 10s | 5 |
| 3 | medium_load | 5,000 | 15s | 20 |
| 4 | high_load | 10,000 | 20s | 50 |
| 5 | stress_test | 50,000 | 30s | 100 |

Each tier includes a 5-second cooldown before the next. The final JSON report includes per-tier results, resource usage (CPU/memory via psutil), and a pass/fail verification against target RPS and error rate thresholds.

## How to Run

All commands use Docker. Nothing runs on the host.

```bash
make build         # Build all Docker images
make test          # Run unit + integration tests
make run           # Start the server (detached)
make load          # Run load generator (1000 logs, 5 workers)
make benchmark     # Run full benchmark suite (5 tiers)
make perf          # Performance run with resource limits
make stop          # Stop all services
make clean         # Remove images and volumes
```

### Typical Workflows

**Quick smoke test:**
```bash
make run && sleep 3 && make load && make stop
```

**Full benchmark suite** (~2 minutes):
```bash
make run && sleep 3 && make benchmark && make stop
```

**Performance run with Docker resource limits** (CPU caps, memory limits, ulimits):
```bash
make perf && make stop
```

## Configuration

### Server Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `SERVER_HOST` | `0.0.0.0` | Bind address |
| `SERVER_PORT` | `9000` | TCP listen port |
| `BUFFER_SIZE` | `65536` | Read buffer size in bytes |
| `ENABLE_TLS` | `false` | Enable TLS encryption |
| `CERT_DIR` | `./certs` | Directory containing TLS certificates |
| `ENABLE_UDP` | `false` | Enable secondary UDP listener |
| `UDP_PORT` | `9001` | UDP listen port |
| `ENABLE_PERSISTENCE` | `true` | Write logs to disk |
| `LOG_DIR` | `./logs` | Directory for persisted log files |
| `MIN_LOG_LEVEL` | `DEBUG` | Minimum log level to accept |
| `CIRCUIT_BREAKER_ENABLED` | `false` | Enable circuit breaker |
| `BATCH_SIZE` | `500` | Flush after this many buffered messages |
| `BATCH_FLUSH_MS` | `100` | Flush after this many milliseconds |

### Generator Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `SERVER_HOST` | `localhost` | Target server address |
| `SERVER_PORT` | `9000` | Target server port |
| `TOTAL_LOGS` | `1000` | Number of log messages to send |
| `DURATION_SECS` | `10` | Maximum test duration in seconds |
| `CONCURRENCY` | `5` | Number of concurrent workers |
| `ENABLE_TLS` | `false` | Use TLS for connections |
| `CA_CERT_PATH` | `./certs/ca.crt` | Path to CA certificate |
| `COMPRESS` | `false` | Enable gzip compression |
| `BATCH_SIZE` | `100` | Messages per batch |
| `TARGET_RPS` | `0` | Target requests per second (0 = unlimited) |

## File Structure

```
load-generator-testing/
├── Makefile                      # All Docker commands
├── docker-compose.yml            # Server, tests, generator, benchmark services
├── docker-compose.perf.yml       # Override with CPU/memory limits for perf runs
├── Dockerfile.server             # Server image
├── Dockerfile.generator          # Generator + benchmark image
├── Dockerfile.test               # Test runner image
├── generate_certs.sh             # Self-signed TLS cert generation
├── pyproject.toml                # Python project config
├── requirements.txt              # Dependencies (psutil)
├── server/
│   ├── __init__.py
│   ├── config.py                 # ServerConfig dataclass (env vars)
│   ├── main.py                   # Entry point: starts TCP server
│   ├── tcp_server.py             # asyncio.start_server + TLS setup
│   ├── handler.py                # NDJSON message parsing + gzip decompression
│   ├── persistence.py            # Batched log writer (flush by count or time)
│   ├── circuit_breaker.py        # Circuit breaker state machine
│   └── udp_server.py             # Optional UDP transport
├── generator/
│   ├── __init__.py
│   ├── config.py                 # GeneratorConfig dataclass (env vars)
│   ├── main.py                   # CLI entry point with argparse
│   ├── load_generator.py         # Async workers, connection pool, progress
│   ├── connection_pool.py        # asyncio.Queue-backed TCP connection pool
│   └── metrics.py                # Latency tracking + percentile computation
├── benchmark/
│   ├── __init__.py
│   ├── main.py                   # CLI entry point for benchmark
│   ├── runner.py                 # Progressive test suite (5 tiers)
│   └── reporter.py               # JSON report generation + resource monitoring
└── tests/
    ├── __init__.py
    ├── test_handler.py           # NDJSON handler unit tests
    ├── test_circuit_breaker.py   # Circuit breaker state machine tests
    ├── test_metrics.py           # Metrics + percentile tests
    └── test_integration.py       # Full server<->client integration tests
```

## What I Learned

- **asyncio vs threads:** asyncio handles thousands of concurrent connections on a single thread via cooperative scheduling, whereas thread-per-client models hit OS thread limits and context-switch overhead much sooner.
- **Connection pooling via asyncio.Queue:** Reusing TCP connections through a queue-based pool eliminates per-message TCP handshake overhead. The pool lazily creates connections up to a limit, then blocks until one is returned.
- **Circuit breaker pattern:** A state machine (closed -> open -> half-open) that trips after repeated failures, preventing cascade failures under overload. The half-open state lets a single request through to probe recovery.
- **Batch persistence:** Flushing every N messages or M milliseconds (whichever comes first) is critical for write throughput. Without batching, each message triggers a disk write and throughput drops by orders of magnitude.
- **Docker resource limits:** `cpus`, `mem_limit`, and `ulimits` in Docker Compose simulate production constraints. The `nofile` ulimit (65536) is essential for high-concurrency tests that open thousands of file descriptors.
- **Tail latency:** P50/P95/P99 percentiles reveal tail latency behavior that is completely invisible in averages. A 2ms average can hide a 200ms P99, which is what real users experience during load spikes.
